<!DOCTYPE html>
<html lang="en">
<body>
  <h1>Creating a K8s Cluster</h1>
  <p>This guide outlines the steps to set up a Kubernetes cluster on AWS with one control plane and two worker nodes. It covers AWS account setup, provisioning infrastructure, installing required packages, configuring containerd, setting up Kubernetes components (kubeadm, kubelet, kubectl), initializing the cluster, and joining worker nodes.</p>
  
  <hr>

  <h2>1. AWS Account and IAM Setup</h2>
  <ul>
    <li><strong>Create an AWS account.</strong></li>
    <li>
      <strong>Create an Admin User with Limited Privileges:</strong>
      <ul>
        <li>Navigate to <em>IAM &gt; Users &gt; Create User</em>.</li>
        <li>Enter a username and enable <strong>AWS Management Console access</strong>.</li>
        <li>Choose an <strong>Autogenerated Password</strong>.</li>
        <li>Click <strong>Next</strong>.</li>
        <li>Under <em>Permissions Options</em>, select <strong>Attach Policy</strong>.</li>
        <li>Attach the <strong>AdministratorAccess</strong> policy.</li>
        <li>Click <strong>Next</strong> and <strong>Create User</strong>.</li>
        <li><strong>Copy the Console sign-in details.</strong></li>
      </ul>
    </li>
    <li><strong>Login</strong> as the Admin User.</li>
  </ul>

  <hr>

  <h2>2. Provision Infrastructure on AWS</h2>
  
  <h3>2.1 Creating the Control Plane</h3>
  <ul>
    <li>Navigate to <strong>EC2 Instances</strong> and click <strong>Launch Instance</strong>.</li>
    <li><strong>Name:</strong> <code>Control-Plane</code></li>
    <li><strong>AMI:</strong> Ubuntu</li>
    <li><strong>Instance Type:</strong> <code>t2.medium</code> (or larger based on your application needs).</li>
    <li><strong>Key Pair:</strong> Create a new key-pair and download the <code>.pem</code> file locally.</li>
    <li>Leave other settings as default.</li>
    <li>Click <strong>Launch Instance</strong>.</li>
  </ul>
  
  <h3>2.2 Creating Worker Nodes</h3>
  <ul>
    <li>Click <strong>Launch Instance</strong> again.</li>
    <li><strong>AMI:</strong> Ubuntu</li>
    <li><strong>Instance Type:</strong> <code>t2.medium</code>.</li>
    <li><strong>Key Pair:</strong> Use the key-pair created for the Control-Plane.</li>
    <li><strong>Number of Instances:</strong> <code>2</code>.</li>
    <li>Name the instances: <code>Worker-Node-1</code> and <code>Worker-Node-2</code>.</li>
    <li>Click <strong>Launch Instances</strong>.</li>
  </ul>

  <hr>

  <h2>3. Connect to the Control Plane Node</h2>
  <ol>
    <li><strong>Set Permissions for the Key-Pair:</strong>
      <pre><code>chmod 400 k8s.pem</code></pre>
    </li>
    <li><strong>Connect via SSH:</strong>
      <pre><code>ssh -i k8s.pem ubuntu@&lt;PUBLIC-IP-ADDRESS&gt;</code></pre>
    </li>
  </ol>

  <hr>

  <h2>4. Kubernetes Cluster Installation Steps</h2>
  
  <h3>4.1 Overview</h3>
  <p>Install the following on <strong>all nodes</strong>:</p>
  <ul>
    <li>Container Runtime</li>
    <li>Kubelet</li>
    <li>Kube Proxy</li>
  </ul>
  <p>Install the following on the <strong>Control Plane Node</strong>:</p>
  <ul>
    <li>API Server</li>
    <li>Scheduler</li>
    <li>Controller Manager</li>
    <li>Etcd</li>
  </ul>

  <h3>4.2 Disable Swap on the Control Plane</h3>
  <p>Execute the following on the control plane node:</p>
  <pre><code>sudo swapoff -a</code></pre>
  <p><strong>Note:</strong> Disabling swap is necessary because memory swapping can lead to performance and stability issues with Kubernetes.</p>

  <hr>

  <h2>5. Opening Ports on Control Plane and Worker Nodes</h2>
  <ol>
    <li>
      <strong>Control Plane Security Group:</strong>
      <ul>
        <li>Go to the <strong>Control-Plane Security Group</strong> in the AWS console.</li>
        <li>Click <strong>Edit inbound rules</strong>.</li>
        <li>Add the necessary control plane ports.</li>
      </ul>
    </li>
    <li>
      <strong>Worker Node Security Group:</strong>
      <ul>
        <li>Apply the same rules as the control plane.</li>
      </ul>
    </li>
  </ol>
  <p><strong>Note:</strong> The VPC CIDR block is <code>172.31.0.0/16</code>.</p>
  
  <p><strong>Example Screenshots:</strong></p>
  <ul>
    <li>
      <strong>Control-Plane-node-SG:</strong><br>
      <img src="https://github.com/user-attachments/assets/617b2f43-5749-40d9-9a6d-06aaa18e3265" alt="Control Plane SG">
    </li>
    <li>
      <strong>Worker-Node-SG:</strong><br>
      <img src="https://github.com/user-attachments/assets/4df27e75-c196-4876-8e35-3ac672264bc6" alt="Worker Node SG">
    </li>
  </ul>

  <hr>

  <h2>6. Set Hostnames for All Nodes</h2>
  <ol>
    <li>
      <strong>On the Control Plane Node:</strong>
      <p>Edit the <code>/etc/hosts</code> file:</p>
      <pre><code>sudo vim /etc/hosts</code></pre>
      <p>Add the private IP addresses:</p>
      <pre><code>172.31.6.192 control-plane
172.31.12.155 workernode-1
172.31.3.144 workernode-2</code></pre>
    </li>
    <li>
      <strong>Set the Hostname:</strong>
      <p>On the control plane node:</p>
      <pre><code>sudo hostnamectl set-hostname control-plane</code></pre>
      <p>On each worker node, run:</p>
      <pre><code>sudo hostnamectl set-hostname &lt;worker-node-name&gt;</code></pre>
    </li>
    <li>
      <strong>Re-login</strong> to update the hostname.
    </li>
  </ol>

  <hr>

  <h2>7. Install and Configure Container Runtime (containerd)</h2>
  
  <h3>7.1 Configure Kernel Modules and sysctl</h3>
  <p>Create the module configuration file:</p>
  <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF</code></pre>
  <p>Load the modules:</p>
  <pre><code>sudo modprobe overlay
sudo modprobe br_netfilter</code></pre>
  <p>Set the sysctl parameters:</p>
  <pre><code>cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF</code></pre>
  <p>Apply the parameters:</p>
  <pre><code>sudo sysctl --system</code></pre>
  
  <h3>7.2 Install containerd</h3>
  <pre><code>sudo apt-get update
sudo apt-get install containerd</code></pre>
  
  <h3>7.3 Customize containerd Configuration</h3>
  <p>Generate and update the default configuration:</p>
  <pre><code>sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml</code></pre>
  <p>Check containerd status:</p>
  <pre><code>sudo systemctl status containerd</code></pre>
  <p><strong>Note:</strong> Repeat these steps on the worker nodes. Consider creating a script (e.g., <code>install-containerd.sh</code>) to automate this on each worker node.</p>
  
  <hr>

  <h2>8. Install Kubernetes Components (kubeadm, kubelet, kubectl)</h2>
  
  <h3>8.1 On the Control Plane Node</h3>
  <ol>
    <li>
      <strong>Update and Install Prerequisites:</strong>
      <pre><code>sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg</code></pre>
    </li>
    <li>
      <strong>Download and Install the Public Signing Key:</strong>
      <pre><code>curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg</code></pre>
    </li>
    <li>
      <strong>Add the Kubernetes apt Repository:</strong>
      <pre><code>echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list</code></pre>
    </li>
    <li>
      <strong>Install Kubernetes Components:</strong>
      <pre><code>sudo apt-get update
sudo apt-get install -y kubelet=1.28.0-1.1 kubeadm=1.28.0-1.1 kubectl=1.28.0-1.1
sudo apt-mark hold kubelet kubeadm kubectl</code></pre>
    </li>
  </ol>
  
  <h3>8.2 On Worker Nodes</h3>
  <p>Create an installation script (e.g., <code>installation-k8s-components.sh</code>) with the above steps and execute it on each worker node.</p>
  
  <hr>

  <h2>9. Initialize the Cluster with kubeadm</h2>
  <p>On the control plane node, initialize the cluster:</p>
  <pre><code>sudo kubeadm init</code></pre>
  
  <h3>9.1 Set Up <code>kubeconfig</code> for <code>kubectl</code></h3>
  <ol>
    <li>
      Switch to the root user:
      <pre><code>sudo -i
export KUBECONFIG=/etc/kubernetes/admin.conf</code></pre>
    </li>
    <li>
      Create a kube configuration folder for your regular user:
      <pre><code>mkdir -p ~/.kube
sudo cp -i /etc/kubernetes/admin.conf ~/.kube/config</code></pre>
    </li>
    <li>
      Change ownership to your user:
      <pre><code>sudo chown $(id -u):$(id -g) ~/.kube/config</code></pre>
    </li>
    <li>
      Verify the configuration:
      <pre><code>ls -l ~/.kube/config</code></pre>
    </li>
  </ol>
  
  <hr>

  <h2>10. Install Cilium Network</h2>
  <ol>
    <li>
      <strong>Download the Cilium CLI:</strong>
      <pre><code>CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}</code></pre>
    </li>
    <li>
      <strong>Install Cilium:</strong>
      <pre><code>cilium install --set ipam.operator.clusterPoolIPv4PodCIDRList="10.0.0.0/9"</code></pre>
    </li>
    <li>
      <strong>Check Cilium Status:</strong>
      <p>Example status output:</p>
      <img src="https://github.com/user-attachments/assets/5cae0f7f-f073-4353-a7c2-9111c65ff06c" alt="Cilium Status">
    </li>
  </ol>
  
  <hr>

  <h2>11. Join Worker Nodes to the Cluster</h2>
  <ol>
    <li>
      <strong>Generate the Join Command on the Control Plane:</strong>
      <pre><code>kubeadm token create --print-join-command</code></pre>
      <p>This outputs a command similar to:</p>
      <pre><code>kubeadm join 172.31.6.192:6443 --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</code></pre>
    </li>
    <li>
      <strong>Execute the Join Command on Each Worker Node:</strong>
      <pre><code>sudo kubeadm join 172.31.6.192:6443 --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</code></pre>
    </li>
  </ol>

  <hr>

  <h2>12. Final Security Rules Update</h2>
  <ul>
    <li><strong>Control-Plane Security Group:</strong> Update rules to allow traffic between any pod in the cluster.</li>
    <li><strong>Worker Node Security Groups:</strong> Apply the same rules as the control plane.</li>
  </ul>

  <hr>

  <h2>13. Verification</h2>
  <ul>
    <li>Check that the worker nodes have joined the cluster:
      <pre><code>kubectl get nodes</code></pre>
    </li>
    <li>Verify pod-to-pod connectivity and overall cluster health.</li>
  </ul>

  <hr>

  <h2>Technologies Used</h2>
  <div class="logo-container">
    <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/kubernetes/kubernetes-plain.svg" alt="Kubernetes Logo" height="40">
    <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/bash/bash-original.svg" alt="Bash Logo" height="40">
  </div>

  <hr>

  <p>This documentation serves as a comprehensive step-by-step guide to setting up your Kubernetes cluster on AWS. Feel free to adjust any details or scripts to suit your environment.</p>
</body>
</html>
